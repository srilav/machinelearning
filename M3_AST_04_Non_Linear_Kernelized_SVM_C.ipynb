{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "M3_AST_04_Non_Linear_Kernelized_SVM_C.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srilav/machinelearning/blob/main/M3_AST_04_Non_Linear_Kernelized_SVM_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84dI5dYEJyzn"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "## A program by IISc and TalentSprint\n",
        "### Assignment 4: Non-linear/Kernelized SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6KrvbwxJyzp"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QenOWxTGJyzq"
      },
      "source": [
        "At the end of the experiment, you will be able to\n",
        "\n",
        "* distinguish between linear and non-linear separable data\n",
        "* understand what is Kernel\n",
        "* know different types of Kernels\n",
        "* perform SVM on non-linear separable data\n",
        "* understand the significance of C and gamma hyperparameter for regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAIKq0VuJyzq"
      },
      "source": [
        "### Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtgSAGDrJyzr"
      },
      "source": [
        "**Quick overview of SVM:**\n",
        "\n",
        "* SVM assumes that the data is linearly separable.\n",
        "\n",
        "* It chooses the line which is more distant from both the classes.\n",
        "\n",
        "In the SVM algorithm, we find the points closest to the line from both the classes. These points are called **support vectors**. We compute the distance between the line and the support vectors, this distance is called the **margin**. The goal is to maximize the margin. The hyperplane for which the margin is maximum is called an **optimal hyperplane**.\n",
        "<br><br>\n",
        "<center>\n",
        "<img src= \"https://cdn.talentsprint.com/aiml/aiml_2020_b14_hyd/experiment_details_backup/linear_data.png\" width= 400 px/>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YzfoPvJDiTX"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjoZJWGErxGf"
      },
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBPPuGmBlDIN",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook= \"M3_AST_04_Non_Linear_Kernelized_SVM_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")  \n",
        "    \n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "    \n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None        \n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "    \n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None   \n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://cds.iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "    \n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional: \n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional  \n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "  \n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "  \n",
        "  \n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "  \n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getId():\n",
        "  try: \n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup \n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup() \n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsm40YexJyzs"
      },
      "source": [
        "### Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJqeKB4RJyzt"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from mlxtend.plotting import plot_decision_regions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJORM_c5Jyzt"
      },
      "source": [
        "### How SVM works with linear and non-linear separable data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E15j94wdJyzu"
      },
      "source": [
        "Linear separable data\n",
        "\n",
        "*  Find the extreme points of the dataset, which are called support vectors.\n",
        "*  Get the optimal hyperplane that has the highest margin between the line and support vectors\n",
        "\n",
        "Non-linear and inseparable data\n",
        "\n",
        "*  Use **Kernels** to transform data into higher dimensions.\n",
        "*  Separate the points using Linear SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdPswtvrJyzu"
      },
      "source": [
        "### Kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAaijWE6Jyzu"
      },
      "source": [
        "The kernel means transforming data into another dimension that has a clear dividing margin between classes of data as shown in the figure below.\n",
        "\n",
        "<center>\n",
        "<img src= \"https://cdn.talentsprint.com/aiml/Experiment_related_data/IMAGES/kernel.png\" width= 500 px/>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8FuV8Q_Jyzv"
      },
      "source": [
        "### Kernelized SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWkNbHgKJyzv"
      },
      "source": [
        "The objective function for linear SVM is given by\n",
        "\n",
        "$$L(\\alpha) = \\sum \\limits_{i=1}^{m} \\alpha_i - \\frac{1}{2} \\sum \\limits_{i=1}^{m} \\sum \\limits_{j=1}^{m} \\alpha_i\\alpha_jt_it_jx_i^T.x_j$$\n",
        "\n",
        "The objective function for linear SVM after $\\phi$ transformation\n",
        "\n",
        "$$L(\\alpha) = \\sum \\limits_{i=1}^{m} \\alpha_i - \\frac{1}{2} \\sum \\limits_{i=1}^{m} \\sum \\limits_{j=1}^{m} \\alpha_i\\alpha_jt_it_j\\phi (x_i)^T.\\phi (x_j)$$\n",
        "\n",
        "where $\\alpha$ is the Lagrange multiplier such that $\\alpha_i \\geq 0$ for $i = 1, \\cdots, m$ and $\\sum \\limits_{i=1}^{m}\\alpha_it_i = 0$.\n",
        "\n",
        "The objective function after transformation is more expensive to evaluate than the previous one and it leads to the following modified form\n",
        "\n",
        "$$L(\\alpha) = \\sum \\limits_{i=1}^{m} \\alpha_i - \\frac{1}{2} \\sum \\limits_{i=1}^{m} \\sum \\limits_{j=1}^{m} \\alpha_i\\alpha_jt_it_jK(x_i,x_j)$$\n",
        "\n",
        "where $K(x_i,x_j) = \\phi (x_i)^T.\\phi (x_j)$ is called a kernel function.\n",
        "\n",
        "Hence, in machine learning, a kernel is a function capable of computing the dot product $ϕ(x_i)^T ϕ(x_j)$ based only on the original vectors $x_i$ and $x_j$, without having to compute (or even to know about) the transformation $ϕ$. This is the essence of the kernel trick.\n",
        "\n",
        "Some of the most commonly used kernels are:\n",
        "\n",
        "* Linear: \n",
        "$$K(x_i, x_j) = x_i^Tx_j$$\n",
        "* Polynomial: \n",
        "$$K(x_i, x_j) = (\\gamma x_i^Tx_j + r)^d$$\n",
        "* Gaussian Radial Basis Function (RBF): \n",
        "$$K(x_i, x_j) = exp(-\\gamma ||x_i - x_j||^2)$$\n",
        "* Sigmoid: \n",
        "$$K(x_i, x_j) = tanh(\\gamma x_i^Tx_j + r)$$\n",
        "\n",
        "where $x_i$ and $x_j$ are original vectors, $d$ is degree of polynomial, $r$ is free parameter and $\\gamma$ is regularization parameter.\n",
        "\n",
        "One of the most commonly used kernels for SVM is RBF kernel. Let's see how it works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MEVRbmbJyzw"
      },
      "source": [
        "### Working of RBF Kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gm4QIagHJyzw"
      },
      "source": [
        "One of the techniques to tackle nonlinear problems is to add features computed using a similarity function that measures how much each instance resembles a particular landmark. \n",
        "\n",
        "For example, let’s take the 1D dataset add two landmarks to it at $x_1  = –2$ and $x_1  = 1$ (see left plot in the figure below). Next, let’s define the similarity function to be the Gaussian Radial Basis Function (RBF) with $\\gamma = 0.3$.\n",
        "\n",
        "$$\\phi_{\\gamma}(x, l) = exp(-\\gamma||x-l||^2)$$\n",
        "\n",
        "where, $l$ is the landmark (which can be another datapoint).\n",
        "\n",
        "It is a bell-shaped function varying from $0$ (very far away from the landmark) to $1$ (at the landmark). \n",
        "\n",
        "Now we compute the new features, let’s look at the instance $x_1  = –1$: it is located at a distance of $1$ from the first landmark, and $2$ from the second landmark. Therefore its new features are \n",
        "* $x_2 = exp (–0.3 × 1^2 ) ≈ 0.74$ and \n",
        "* $x_3 = exp (–0.3 × 2^2) ≈ 0.30$. \n",
        "\n",
        "The plot on the right of the below figure shows the transformed dataset. As we can see, it is now linearly separable.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/RBFKernel_plots.png\" />\n",
        "<figcaption>Feature space before transformation$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $Feature space after transformation</figcation>\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "**How to select the landmarks**\n",
        "\n",
        "The simplest approach is to create a landmark at the location of every instance in the dataset. This creates many dimensions and thus increases the chances that the transformed training set will be linearly separable. The downside is that if the training set is very large, we end up with an equally large number of features and it may be computationally expensive to compute all the additional features. However, the kernel trick does its SVM magic: it makes it possible to obtain a similar result as if we had added many similarity features, without actually having to add them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jPn1lq3Jyzw"
      },
      "source": [
        "**Exercise 1:** Perform transformation on the `circles` dataset of Scikit-Learn to make the classes linearly separable using Radial Basis Function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycx4eR4EJyzx"
      },
      "source": [
        "Load the data from the Scikit-Learn datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6rq9IH8Jyzx"
      },
      "source": [
        "# The number of points generated is 100 \n",
        "# The Standard deviation of Gaussian noise added to the data is 0.1\n",
        "\n",
        "X, y = datasets.make_circles(100, factor=0.1, noise=.1)\n",
        "# Scale the data\n",
        "X = StandardScaler().fit_transform(X)\n",
        "# Visualize the data\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb8BskpXJyzy"
      },
      "source": [
        "Mapping Function (Radial basis function) \n",
        "\n",
        "* By using Radial basis function add one more dimension to the original data to visualize the data linearly in high dimensional space\n",
        "* Below is the formula to compute RBF function. The gamma is the regularization parameter. Here take gamma = 1\n",
        "\n",
        "   $K(X, X_i) = exp(-\\gamma  \\sum(X-X_i)^2)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVqP51XaJyzy"
      },
      "source": [
        "# Radial Basis Function where gamma = 1 and taking landmark at (0,0)\n",
        "gamma = 1\n",
        "landmark = np.array([0.0, 0.0])\n",
        "rbf = np.exp(-gamma * np.sum((X - landmark)**2, axis = 1))\n",
        "print(rbf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pCf05N9Jyzz"
      },
      "source": [
        "# Visualze data in 3d\n",
        "\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "ax = plt.axes(projection='3d')\n",
        "ax.scatter(X[:, 0], X[:, 1], rbf, c=y, s=20, cmap='autumn')\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-10Rq4dyJyzz"
      },
      "source": [
        "From the above plot, we can observe that the data becomes linearly separable by transforming the data to higher dimensions.\n",
        "\n",
        "Here we specify the landmark based on intuition but in practice, SVM takes care of it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgtrKF6PJyzz"
      },
      "source": [
        "Now, we apply SVM Classifier using RBF Kernel.\n",
        "\n",
        "In Scikit-Learn, we can apply kernelized SVM simply by specifying kernel to RBF (radial basis function) kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMfdrEl2Jyz0"
      },
      "source": [
        "# Kernel is 'rbf'\n",
        "clf_RBF = SVC(kernel='rbf').fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Srhp-mmAJyz0"
      },
      "source": [
        "# Visualization using RBF Kernel\n",
        "plot_decision_regions(X, y, clf_RBF, legend=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxKKmPb-Jyz0"
      },
      "source": [
        "Using the RBF kernelized support vector machine, we see a suitable nonlinear decision boundary.\n",
        "\n",
        "Another approach to handle nonlinear datasets is to add more features, such as polynomial features; in some cases this can result in a linearly separable dataset. Let's see how Polynomial Kernel works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vimF-6hZJyz0"
      },
      "source": [
        "### Working of Polynomial Kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLc_zXEfJyz1"
      },
      "source": [
        "Consider the left plot in the figure below: it represents a simple dataset with just one feature, $X_1$. As we can see, this dataset is not linearly separable. But if we add a second feature $X_2 = (X_1)^2$, the resulting 2D dataset is perfectly linearly separable as shown in the plot on right.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.talentsprint.com/aiml/aiml_2020_b14_hyd/experiment_details_backup/non_linear_data.png\" width= 600 px/>\n",
        "</center>\n",
        "\n",
        "Adding polynomial features is simple to implement and can work great with all sorts of Machine Learning algorithms (not just SVMs). Also, at a low polynomial degree, this method cannot deal with very complex datasets, and with a high polynomial degree, it creates a huge number of features, making the model too slow. Fortunately, when using SVMs we can apply an almost miraculous mathematical technique called the kernel trick. The kernel trick makes it possible to get the same result as if we had added many polynomial features, even with very high-degree polynomials, without actually having to add them. So there is no combinatorial explosion of the number of features because we don’t actually add any features. This trick is implemented by the `SVC` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yVCSWwXJyz1"
      },
      "source": [
        "**Exercise 2:** Perform transformation on the `circles` dataset of Scikit-Learn to make the classes linearly separable by\n",
        "* manually selecting higher dimension features\n",
        "* applying Polynomial Kernel of SVC class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0IuHk3fJyz2"
      },
      "source": [
        "Load the data from the Scikit-Learn datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHS5XbgdJyz2"
      },
      "source": [
        "# The number of points generated is 100 \n",
        "# The Standard deviation of Gaussian noise added to the data is 0.1\n",
        "\n",
        "X, y = datasets.make_circles(100, factor=0.1, noise=.1)\n",
        "# Scale the data\n",
        "X = StandardScaler().fit_transform(X)\n",
        "# Vicualize the data\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylwzyTqzJyz2"
      },
      "source": [
        "# For each instance X=[a,b], let's consider three features X = [a^2, b^2, a*b]\n",
        "feature_1 = X[:, 0]**2\n",
        "feature_2 = X[:, 1]**2\n",
        "feature_3 = X[:, 0] * X[:, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MB-H5rIzJyz2"
      },
      "source": [
        "Visulaization of data in 3D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXcm2hSwJyz3"
      },
      "source": [
        "# Visualzing in 3d\n",
        "\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "ax = plt.axes(projection='3d')\n",
        "ax.scatter(feature_1, feature_2, feature_3, c=y, s=20, cmap='autumn')\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KGrMcz1Jyz3"
      },
      "source": [
        "From the above plot, we can observe that the data becomes linearly separable by transforming the data to higher dimensions.\n",
        "\n",
        "Now, we apply SVM Classifier using Polynomial Kernel and visualize the decision boundary.\n",
        "\n",
        "In Scikit-Learn, we can apply kernelized SVM simply by specifying kernel to `poly` kernel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Okx4Ip7Jyz3"
      },
      "source": [
        "# Kernel is 'poly'\n",
        "clf_poly = SVC(kernel='poly', degree=2).fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUfGgXsPJyz4"
      },
      "source": [
        "Visualization using Polynomial Kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eym4wI4pJyz4"
      },
      "source": [
        "plot_decision_regions(X, y, clf_poly, legend=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdB8oFbkJyz4"
      },
      "source": [
        "Using the polynomial kernelized support vector machine also, we see a suitable non-linear decision boundary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJZCzkIMJyz4"
      },
      "source": [
        "**Let's see how we can make classification using linear, polynomial, and rbf kernel of SVM on different datasets.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p-orDsDJyz4"
      },
      "source": [
        "**Exercise 3**: Perform classification on `moons` dataset of Scikit-Learn using different kernels and compare their prediction accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrOgNcYtJyz6"
      },
      "source": [
        "Load the data from the Scikit-Learn datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmvkFkGJJyz6"
      },
      "source": [
        "# The number of points generated is 100 \n",
        "# The Standard deviation of Gaussian noise added to the data is 0.1\n",
        "\n",
        "X, y = datasets.make_moons(100, noise=.1)\n",
        "# Scale the data\n",
        "X = StandardScaler().fit_transform(X)\n",
        "# Visualize the data\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2blb8TKCJyz6"
      },
      "source": [
        "Splitting the data into training and testing set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JLqDpXTJyz7"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3, random_state= 123)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5wTthANJyz7"
      },
      "source": [
        "Apply the SVM classifier and try to fit the model using **Linear Kernel** on the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WM-hG3WiJyz7"
      },
      "source": [
        "clf = SVC(kernel='linear')\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H7gfoKXJyz8"
      },
      "source": [
        "Let us visualize the decision boundaries of the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3d0g_QnJyz8"
      },
      "source": [
        "# Function for plotting Decision boundaries\n",
        "\n",
        "def plot_svc_decision_boundaries(model, ax=None):\n",
        "    \n",
        "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
        "    if ax is None:\n",
        "        ax = plt.gca()                  # get the current Axes instance on the current figure matching the given keyword arguments, or create one\n",
        "    xlim = ax.get_xlim()                # return the x-axis view limits\n",
        "    ylim = ax.get_ylim()                # return the y-axis view limits\n",
        "    \n",
        "    # create grid to evaluate model\n",
        "    x = np.linspace(xlim[0], xlim[1], 30)\n",
        "    y = np.linspace(ylim[0], ylim[1], 30)\n",
        "    Y, X = np.meshgrid(y, x)\n",
        "    xy = np.vstack([X.reshape(-1), Y.reshape(-1)]).T\n",
        "    # Evaluates the decision function for the samples in X.\n",
        "    P = model.decision_function(xy).reshape(X.shape)  \n",
        "    \n",
        "    # plot decision boundary and margins\n",
        "    ax.contour(X, Y, P, colors='k',\n",
        "               levels=[-1, 0, 1], alpha=0.5,\n",
        "               linestyles=['--', '-', '--'])\n",
        "    \n",
        "    ax.set_xlim(xlim)\n",
        "    ax.set_ylim(ylim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47hAywZyJyz8"
      },
      "source": [
        "Call the above function by passing the trained model to observe the decision boundaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITxqL4bjJyz8"
      },
      "source": [
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plot_svc_decision_boundaries(clf);\n",
        "# Visualize the support vectors\n",
        "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
        "             s=200, lw=1, facecolors='green', alpha=0.3);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMt5tHe9Jyz9"
      },
      "source": [
        "y_pred = clf.predict(X_test)\n",
        "acc_linear = accuracy_score(y_test, y_pred)\n",
        "print(acc_linear)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-onn4lpXJyz-"
      },
      "source": [
        "Now, we apply SVM Classifier using **Polynomial Kernel**\n",
        "\n",
        "In Scikit-Learn, we apply kernelized SVM simply by changing `linear` kernel to `poly` kernel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkOvy4ijJyz_"
      },
      "source": [
        "# Kernel is 'poly'\n",
        "clf = SVC(kernel='poly', degree= 3, coef0= 1)\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GzHjqPZJyz_"
      },
      "source": [
        "Visualization using Polynomial Kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpEXzbMeJyz_"
      },
      "source": [
        "# Call the decision boundary function \n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plot_svc_decision_boundaries(clf)\n",
        "# Visualize the support vectors\n",
        "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
        "             s=200, lw=1, facecolors='green', alpha=0.3);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-kCCHshJyz_"
      },
      "source": [
        "# Make prediction on test data\n",
        "y_pred = clf.predict(X_test)\n",
        "acc_polynomial = accuracy_score(y_test, y_pred)\n",
        "print(acc_polynomial)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3WcegVPJy0A"
      },
      "source": [
        "Now, we apply SVM Classifier using **RBF Kernel**\n",
        "\n",
        "In Scikit-Learn, apply kernelized SVM by changing `linear` kernel to `rbf` kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghdyRMPVJy0A"
      },
      "source": [
        "# Kernel is 'rbf'\n",
        "clf = SVC(kernel='rbf')\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmCAiFqnJy0A"
      },
      "source": [
        "Visualization using RBF Kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9zigNKjJy0A"
      },
      "source": [
        "# Call the decision boundary function \n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plot_svc_decision_boundaries(clf)\n",
        "# Display support vectors\n",
        "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
        "             s=200, lw=1, facecolors='green', alpha=0.3);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czqN1YDwJy0B"
      },
      "source": [
        "# Make prediction on test data\n",
        "y_pred = clf.predict(X_test)\n",
        "acc_rbf = accuracy_score(y_test, y_pred)\n",
        "print(acc_rbf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvz58s1OJy0B"
      },
      "source": [
        "From the above results, we can see that a non-linear decision boundary can be seen in case of polynomial and rbf kernel. Their testing accuracies are also higher than the linear kernel.\n",
        "\n",
        "Till now we have seen datasets with two classes and two clusters. Let's see how the different kernels perform on the dataset having two classes and four clusters (two clusters per class)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPF1xxjfJy0B"
      },
      "source": [
        "**Exercise 4**: Perform classification on the non-linear separable dataset having 2 classes and 2 clusters per class using different kernels and compare their prediction accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ7i1-eHJy0B"
      },
      "source": [
        "Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgzOcUzwJy0B"
      },
      "source": [
        "# generate 25 random numbers for 4 quadrants represents XOR\n",
        "x1 = np.random.uniform(1,3,25)                 \n",
        "x2 = np.random.uniform(4,6,25)                 \n",
        "y1 = np.random.uniform(1,3,25)                 \n",
        "y2 = np.random.uniform(4,6,25)                 \n",
        "\n",
        "# features\n",
        "X_1 = np.vstack([np.append(x1,x2), np.append(y1,y2)]).T  \n",
        "X_2 = np.vstack([np.append(x1,x2), np.append(y2,y1)]).T  \n",
        "X = np.vstack([X_1, X_2])   \n",
        "\n",
        "# label 0 and 1\n",
        "y_1 = [0 for i in range(len(X_1))]                              \n",
        "y_2 = [1 for i in range(len(X_2))]                                 \n",
        "y = np.append(y_1, y_2)  \n",
        "\n",
        "# stack features and labels\n",
        "data = np.hstack([X,y.reshape(-1,1)])          "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4p-iPGwSKaC"
      },
      "source": [
        "Shuffle the sequential data and separate into features and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoz6RaWESJOt"
      },
      "source": [
        "# shuffle the dataset\n",
        "np.random.shuffle(data)   \n",
        "# Split the data                     \n",
        "X, y = data[:,:2], data[:,2]                   \n",
        "X = StandardScaler().fit_transform(X) \n",
        "\n",
        "# Visualize data\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8PGJi4fnoV0"
      },
      "source": [
        "This example of a nonlinear problem resembles the XOR dataset. We can see that it is impossible to try a single line that can separate the red class from the yellow class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNp31M-uJy0C"
      },
      "source": [
        "Splitting the data into training and testing set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsAwrdPSJy0C"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3, random_state= 123)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9U1vXsHTJy0C"
      },
      "source": [
        "Apply the SVM classifier and try to fit the model using **Linear Kernel** on the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWZXzAcVJy0C"
      },
      "source": [
        "clf = SVC(kernel='linear')\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0N7KwW55Jy0D"
      },
      "source": [
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plot_svc_decision_boundaries(clf);\n",
        "# Visualize the support vectors\n",
        "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
        "             s=200, lw=1, facecolors='green', alpha=0.3);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oM3l4qIlSmrI"
      },
      "source": [
        "predict the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWbfMYowJy0D"
      },
      "source": [
        "# Make prediction on test data\n",
        "y_pred = clf.predict(X_test)\n",
        "acc_linear = accuracy_score(y_test, y_pred)\n",
        "print(acc_linear)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4xYG8ZXJy0D"
      },
      "source": [
        "Now, we apply SVM Classifier using **Polynomial Kernel**\n",
        "\n",
        "In Scikit-Learn, we apply kernelized SVM simply by changing `linear` kernel to `poly` kernel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AePpisbzJy0D"
      },
      "source": [
        "# Kernel is 'poly'\n",
        "clf = SVC(kernel='poly', degree= 3, coef0= 1)\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yG79wyoAJy0E"
      },
      "source": [
        "Visualization using Polynomial Kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pf8ZV4MJy0E"
      },
      "source": [
        "# Call the decision boundary function \n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plot_svc_decision_boundaries(clf)\n",
        "# Visualize the support vectors\n",
        "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
        "             s=200, lw=1, facecolors='green', alpha=0.3);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqZCO7_9SumF"
      },
      "source": [
        "predict the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXRqbelNJy0E"
      },
      "source": [
        "# Make prediction on test data\n",
        "y_pred = clf.predict(X_test)\n",
        "acc_polynomial = accuracy_score(y_test, y_pred)\n",
        "print(acc_polynomial)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWYxM_O3Jy0E"
      },
      "source": [
        "Now, we apply SVM Classifier using **RBF Kernel**\n",
        "\n",
        "In Scikit-Learn, apply kernelized SVM simply by changing linear kernel to an RBF (radial basis function) kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efN60J2CJy0F"
      },
      "source": [
        "# Kernel is 'rbf'\n",
        "clf = SVC(kernel='rbf')\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOnnXVW3Jy0F"
      },
      "source": [
        "Visualization using RBF Kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_himQDvJy0F"
      },
      "source": [
        "# Call the decision boundary function \n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plot_svc_decision_boundaries(clf)\n",
        "# Visualize the support vectors\n",
        "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
        "             s=200, lw=1, facecolors='green', alpha=0.3);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwoY71HRSvdI"
      },
      "source": [
        "predict the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBVLJsT6Jy0F"
      },
      "source": [
        "# Make prediction on test data\n",
        "y_pred = clf.predict(X_test)\n",
        "acc_rbf = accuracy_score(y_test, y_pred)\n",
        "print(acc_rbf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzfmdC0RJy0G"
      },
      "source": [
        "Using these kernelized support vector machines, we learn a suitable nonlinear decision boundary. These kernel transformation strategies are used often in machine learning to turn fast linear methods into fast nonlinear methods, especially for models in which the kernel trick can be used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE_dU1xUJy0G"
      },
      "source": [
        "So far we have centered around very clean datasets, in which a perfect decision boundary exists. But what if the **data has some amount of overlap?** \n",
        "\n",
        "For example, we may have data like below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doOX1e3fJy0G"
      },
      "source": [
        "X1, y1 = datasets.make_blobs(n_samples = 100, centers = 2,\n",
        "                            random_state = 0, cluster_std = 1.2)\n",
        "X1 = StandardScaler().fit_transform(X1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mtf9Qk3GJy0G"
      },
      "source": [
        "To get a sense of the data, let us visualize the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDcTWfbzJy0G"
      },
      "source": [
        "plt.scatter(X1[:, 0], X1[:, 1], c=y1, s=50, cmap='autumn');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9Cp8xplJy0H"
      },
      "source": [
        "To handle this case, the SVM implementation has a bit of a fudge-factor which \"softens\" the margin: that is, it allows some of the points to creep into the margin if that allows a better fit. The hardness of the margin is controlled by a tuning parameter, most often known as C."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd5guix7Jy0H"
      },
      "source": [
        "### Tuning the SVM parameter (C)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D1PtBxEJy0H"
      },
      "source": [
        "For a very large C, the margin is hard, and points cannot lie in it. For smaller C, the margin is softer and can grow to encompass some points.\n",
        "\n",
        "Let us visualize the picture of how a changing C parameter affects the final fit, via the softening of the margin:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sgtv1Lr9Jy0H"
      },
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
        "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
        "\n",
        "for axi, C in zip(ax, [10.0, 0.1]):\n",
        "    model = SVC(kernel='linear', C=C).fit(X1, y1)\n",
        "    axi.scatter(X1[:, 0], X1[:, 1], c=y1, s=50, cmap='autumn')\n",
        "    plot_svc_decision_boundaries(model, axi)\n",
        "    axi.scatter(model.support_vectors_[:, 0],                 # display support vectors\n",
        "                model.support_vectors_[:, 1],\n",
        "                s=200, lw=1, facecolors='green', alpha=0.3);\n",
        "    axi.set_title('C = {0:.1f}'.format(C), size=14)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q0rmvbvJy0H"
      },
      "source": [
        "Note that the kernel used in the above problem was linear. In case of nonlinear SVM, other than C parameter we also have $\\gamma$(gamma) parameter whose relationship with regularization is similar to C parameter, that is, the more the value of $\\gamma$ lesser the regularization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rP8EqIDVJy0I"
      },
      "source": [
        "### Tuning the SVM parameter (gamma, $\\gamma$)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjQTIxT4Jy0I"
      },
      "source": [
        "Let us visualize the picture of how changing $\\gamma$ and C parameters affect the final fit. We will use `moons` dataset for this purpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rSbz2nuJy0I"
      },
      "source": [
        "# Load the dataset\n",
        "X1, y1 = datasets.make_moons(200, noise=.1)\n",
        "X1 = StandardScaler().fit_transform(X1)\n",
        "plt.scatter(X1[:, 0], X1[:, 1], c=y1, s=50, cmap='autumn');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlwT6CcYJy0I"
      },
      "source": [
        "# Visualize the model with different C and gamma hyperparameters\n",
        "fig, [[ax1, ax2], [ax3, ax4]] = plt.subplots(2, 2, figsize=(14, 8))\n",
        "\n",
        "gamma_values = [0.1, 0.1, 5, 5]\n",
        "C_values = [0.001, 1000, 0.001, 1000]\n",
        "for axi, g, C in zip([ax1, ax2, ax3, ax4], gamma_values, C_values):\n",
        "    model = SVC(kernel='rbf', gamma = g, C=C).fit(X1, y1)\n",
        "    axi.scatter(X1[:, 0], X1[:, 1], c=y1, s=50, cmap='autumn')\n",
        "    plot_svc_decision_boundaries(model, axi)\n",
        "    axi.set_title('gamma = {}, C = {}'.format(g, C), size=14)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mc2nWjKkJy0J"
      },
      "source": [
        "The above plots show models trained with different values of hyperparameters gamma ($γ$) and C. Increasing gamma makes the bell-shaped curve narrower (see the lefthand plots). As a result, each instance’s range of influence is smaller: the decision boundary ends up being more irregular, wiggling around individual instances. \n",
        "\n",
        "Conversely, a small gamma value makes the bell-shaped curve wider: instances have a larger range of influence, and the decision boundary ends up smoother. So $γ$ acts like a regularization hyperparameter: if the model is overfitting, we should reduce it; if it is underfitting,\n",
        "we should increase it (similar to the C hyperparameter)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF9k535SJy0J"
      },
      "source": [
        "### Theory Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb2g691XJy0J"
      },
      "source": [
        "1. Should you use the primal or the dual form of the SVM problem to train a model on a training set with millions of instances and hundreds of features?\n",
        "\n",
        "    This question applies only to linear SVMs since kernelized SVMs can only use\n",
        "    the dual form. The computational complexity of the primal form of the SVM\n",
        "    problem is proportional to the number of training instances $m$, while the computational complexity of the dual form is\n",
        "    proportional to a number between $m^2$ and $m^3$. So if there are millions of instances, you should definitely use the \n",
        "    primal form, because the dual form will be much too slow.\n",
        "\n",
        "\n",
        "2. Say you trained an SVM classifier with an RBF kernel. It seems to underfit the training set: should you increase or decrease $γ$ (gamma)? What about C?\n",
        "\n",
        "    If an SVM classifier trained with an RBF kernel underfits the training set, there might be too much regularization. To \n",
        "    decrease it, you need to increase gamma or C (or both)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgSwVENIPcM6"
      },
      "source": [
        "#@title Select the TRUE statement: { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"\" #@param [\"\", \"A kernel function is applied on each data instance to map the original non-linear data points into some higher dimensional space in which they become linearly separable\", \"A kernel function is applied on each data instance to map the original linear data points into some higher dimensional space in which they become linearly inseparable\", \"A kernel function is applied on each data instance to map the original non-linear data points into some lower dimensional space in which they become linearly separable\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzAZHt1zw-Y-",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}