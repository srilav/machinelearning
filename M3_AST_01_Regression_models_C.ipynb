{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M3_AST_01_Regression_models_C.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srilav/machinelearning/blob/main/M3_AST_01_Regression_models_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps9llghv8jX1"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "## A program by IISc and TalentSprint\n",
        "### Assignment 1: Regression Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeP1PAXf8jYD"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkwaW3k58jYG"
      },
      "source": [
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* have an overview of the basics of Machine Learning\n",
        "\n",
        "* understand the implementation of Train/Test Split\n",
        "\n",
        "* develop an understanding of Least Squares, Learning Curves\n",
        "\n",
        "* perform Linear Regression\n",
        "\n",
        "* have an understanding of Regularization of Linear Models \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC7_wvrxhv9o"
      },
      "source": [
        "### Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLVIgHBjeaXx"
      },
      "source": [
        "**Machine learning** is a subfield of artificial intelligence (AI). The goal of machine learning is to understand the structure of data and model(fit) the data so that it can accurately predict the label or output for similar unseen data.\n",
        "\n",
        "**Machine Learning use cases:**\n",
        "\n",
        "Detecting tumors in brain scans, automatically classifying news articles, automatically flagging offensive comments on discussion forums, \n",
        "summarizing long documents automatically, \n",
        "creating a chatbot or a personal assistant, \n",
        "detecting credit card fraud, \n",
        "making your app react to voice commands, \n",
        "building an intelligent bot for a game.\n",
        "\n",
        "**Machine Learning Workflow:**\n",
        "\n",
        "1. Frame the ML problem by looking at the business need \n",
        "2. Gather the data and do Data Munging/Wrangling for each subproblem \n",
        "3. Explore different models, perform V&V and shortlist promising candidates \n",
        "4. Fine-tune shortlisted models and combine them together to form the final  solution \n",
        "5. Present your solution  \n",
        "6. Deploy \n",
        "\n",
        "\n",
        "**Model training and testing**\n",
        "\n",
        "![wget](https://cdn.iisc.talentsprint.com/CDS/Images/model_train_test1.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWDdGhE4miC9"
      },
      "source": [
        "### Training, Validation, and Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHbUne3bm5O5"
      },
      "source": [
        "A machine learning algorithm splits the Dataset into two sets. \n",
        "\n",
        "Splitting your dataset is essential for an unbiased evaluation of prediction performance. In most cases, it’s enough to split your dataset randomly into two subsets:\n",
        "\n",
        "**Training Dataset:** The sample of data used to fit the model.\n",
        "\n",
        "**Test Dataset:** The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.\n",
        "\n",
        "We usually split the data around 20%-80% between the testing and training stages. \n",
        "\n",
        "**Validation Set:** This is a separate section of your dataset that you will use during training to get a sense of how well your model is doing on data that are not being used in training.\n",
        "\n",
        "In less complex cases, when you don’t have to tune hyperparameters, it’s okay to work with only the training and test sets.\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/700/1*aNPC1ifHN2WydKHyEZYENg.png\" alt=\"drawing\" width=\"500\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpQGMY8R3NFY"
      },
      "source": [
        "#### Prerequisites for using train_test_split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KurBaHZL3PPI"
      },
      "source": [
        "We will use scikit-learn, or sklearn which has many packages for data science and machine learning. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6y_C2m97dgv"
      },
      "source": [
        "**Applying train_test_split()**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YKF0nWf7iEm"
      },
      "source": [
        "You need to import:\n",
        "\n",
        "1.   train_test_split()\n",
        "2.   NumPy \n",
        "\n",
        "We import NumPy because, in supervised machine learning applications, you’ll typically work with two such sequences:\n",
        "\n",
        "* A two-dimensional array with the inputs (x)\n",
        "* A one-dimensional array with the outputs (y)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9I5gDbRB1F8"
      },
      "source": [
        "**sklearn.model_selection.train_test_split(arrays, options)**\n",
        "\n",
        "* **arrays** is the sequence of lists, NumPy arrays, pandas DataFrames, or similar array-like objects that hold the data you want to split. All these objects together make up the dataset and must be of the same length.\n",
        "\n",
        "* **options** are the optional keyword arguments that you can use to get desired behavior:\n",
        "\n",
        "  * **train_size** is the number that defines the size of the training set. \n",
        "\n",
        "  * **test_size** is the number that defines the size of the test set. You should provide either train_size or test_size. If neither is given, then the default share of the dataset that will be used for testing is 0.25, or 25 percent.\n",
        "\n",
        "  * **random_state** is the object that controls randomization during splitting. It can be either an int or an instance of RandomState. The default value is None.\n",
        "\n",
        "  * **shuffle** is the Boolean object (True by default) that determines whether to shuffle the dataset before applying the split.\n",
        "\n",
        "  * **stratify** is an array-like object that, if not None, determines how to use a stratified split.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YzfoPvJDiTX"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjoZJWGErxGf"
      },
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBPPuGmBlDIN",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook= \"M3_AST_01_Regression_models_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")  \n",
        "    ipython.magic(\"sx wget 'https://cdn.iisc.talentsprint.com/CDS/Datasets/Real_estate.csv'\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "    \n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None        \n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "    \n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None   \n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://cds.iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "    \n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional: \n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional  \n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "  \n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "  \n",
        "  \n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "  \n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getId():\n",
        "  try: \n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup \n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup() \n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyV0gnaQV4GZ"
      },
      "source": [
        "### Importing required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GorV3kJHV3gC"
      },
      "source": [
        "# Importing Standard Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Importing sklearn Libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline \n",
        "from sklearn.model_selection import train_test_split, learning_curve\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_28riB2i-z_"
      },
      "source": [
        "### Let us use a small dataset to understand how to implement a train and test split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDgLrsIHD8Q3"
      },
      "source": [
        "#### Creating a simple dataset to work with"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VWTQUroD8ak"
      },
      "source": [
        "# inputs in the two-dimensional array X \n",
        "X = np.arange(1, 25).reshape(12, 2)\n",
        "\n",
        "# outputs in the one-dimensional array y\n",
        "y = np.array([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URRI4cnPD8rn"
      },
      "source": [
        "print(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYn2VwOBEkox"
      },
      "source": [
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcEUV1HkE-W0"
      },
      "source": [
        "#### Splitting input and output datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juSLomf0E-lL"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=4, random_state=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-o4KrAxuFQOl"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWGHP9sRFT4h"
      },
      "source": [
        "X_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZvPfClhFUDM"
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAdX8odIFQUf"
      },
      "source": [
        "y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c01hKgX3UVe8"
      },
      "source": [
        "### Develop an understanding of Least Squares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlO8nnlKYOk4"
      },
      "source": [
        "**Least Squares** method is a statistical procedure to find the best fit for a set of data points by minimizing the sum of the offsets or residuals of points from the plotted curve.\n",
        "\n",
        "**Calculate Line Of Best Fit**\n",
        "\n",
        "A more accurate way of finding the line of best fit is the least square method.\n",
        "\n",
        "Use the following steps to find the equation of line of best fit for a set of ordered pairs $(x_1,y_1),(x_2,y_2),...(x_n,y_n)$.\n",
        "\n",
        "**Step 1:** Calculate the slope ‘m’ by using the following formula:\n",
        "\n",
        "$$m = \\frac{\\sum \\left ( x-\\bar{x} \\right )*\\sum \\left ( y-\\bar{y} \\right )}{\\sum \\left ( x-\\bar{x} \\right )^{2}}$$\n",
        "\n",
        "\n",
        "**Step 2:** Compute the y -intercept of the line by using the formula:\n",
        "\n",
        "$$c = y - mx$$\n",
        "\n",
        "**Step 3:** Substitute the values in the final equation\n",
        "\n",
        "$$y = mx + c$$\n",
        "\n",
        "* y: dependent variable\n",
        "* m: the slope of the line\n",
        "* x: independent variable\n",
        "* c: y-intercept\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WfqM10H8Tqh"
      },
      "source": [
        "As an example, we will try to find the least squares regression line for the below data set:\n",
        "\n",
        "\\begin{array} {|r|r|}\\hline Hours Spent & Grade \\\\\\hline 6 & 82 \\\\ \\hline 10 & 88 \\\\ \\hline 2 & 56 \\\\ \\hline 4 & 64 \\\\ \\hline 6 & 77 \\\\ \\hline 7 & 92 \\\\ \\hline 0 & 23 \\\\ \\hline 1 & 41 \\\\ \\hline 8 & 80 \\\\ \\hline 5 & 59 \\\\ \\hline 3 & 47 \\\\ \\hline  \\end{array}\n",
        "\n",
        "$x$ = HoursSpent\n",
        "\n",
        "$y$ = Grade\n",
        "\n",
        "$\\bar{x}$ = 4.72\n",
        "\n",
        "$\\bar{y}$ = 64.45\n",
        "\n",
        "\n",
        "\\begin{array} {|r|r|}\\hline Hours Spent & Grade &  x - \\bar{x}  & y - \\bar{y} & (x - \\bar{x})*(y - \\bar{y}) \\\\ \\hline 6 & 82 & 1.27 & 17.55 & 22.33 \\\\ \\hline 10 & 88 & 5.27 & 23.55 & 124.15 \\\\ \\hline 2 & 56 & -2.73 & -8.45 & 23.06 \\\\ \\hline 4 & 64 & -0.73 & -0.45 & 0.33 \\\\ \\hline 6 & 77 & 1.27 & 12.55 & 15.97 \\\\ \\hline 7 & 92 & 2.27 & 27.55 & 62.60 \\\\ \\hline 0 & 23 & -4.73 & -41.45 & 195.97 \\\\ \\hline 1 & 41 & -3.73 & -23.42 & 87.42 \\\\ \\hline 8 & 80 & 3.27 & 15.55 & 50.88 \\\\ \\hline 5 & 59 & 0.27 & -5.45 & -1.49 \\\\ \\hline 3 & 47 & -1.73 & -17.45 & 30.15 \\\\ \\hline  \\end{array}\n",
        "\n",
        "\n",
        "$$\\sum \\left ( x-\\bar{x} \\right )*\\sum \\left ( y-\\bar{y} \\right ) = 611.36$$\n",
        "\n",
        "$$\\sum \\left ( x-\\bar{x} \\right )^{2} = 94.18$$\n",
        "\n",
        "$$m = \\frac{611.36}{94.18}$$\n",
        "\n",
        "$$m = 6.49$$\n",
        "\n",
        "**Calculate the intercept:**\n",
        "\n",
        "$$c = y - mx$$\n",
        "\n",
        "$$c = 64.45-(6.49*4.72)$$\n",
        "\n",
        "$$c = 64.45 – 30.63$$\n",
        "\n",
        "$$c = 30.18$$\n",
        "\n",
        "Now that we have all the values to fit into the equation. If we want to know the predicted grade of someone who spends 2.35 hours on their essay, all we need to do is substitute that in for X. \n",
        "\n",
        "$$y =  (6.49 * X) + 30.18 $$\n",
        "\n",
        "$$y = (6.49 * 2.35) + 30.18$$\n",
        "\n",
        "$$y = 45.43$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J5c2tcxCv3y"
      },
      "source": [
        "### Example: Ordinary least squares Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwArexvF29YG"
      },
      "source": [
        "Ordinary least squares (OLS) is a type of linear least squares method for estimating the unknown parameters in a linear regression model. OLS chooses the parameters of a linear function of a set of explanatory variables by the principle of least squares: minimizing the sum of the squares of the differences between the observed dependent variable (values of the variable being observed) in the given dataset and those predicted by the linear function of the independent variable.\n",
        "\n",
        "Geometrically, this is seen as the sum of the squared distances, parallel to the axis of the dependent variable, between each data point in the set and the corresponding point on the regression surface—the smaller the differences, the better the model fits the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUYjc5cSI0gv"
      },
      "source": [
        "# Generating Sample data \n",
        "\n",
        "rng = np.random.RandomState(1)              # instantiate random number generator\n",
        "x = 10 * rng.rand(50)                       # generate 50 random numbers from uniform distribution\n",
        "y = 2 * x - 5 + rng.randn(50)               # use 50 random numbers from normal distribution as noise\n",
        "plt.scatter(x, y, c='b');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWW8fCnnNq3V"
      },
      "source": [
        "**Using Scikit-Learn's Linear Regression estimator to fit the above data and construct the best-fit line**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XVil2eQRLkk"
      },
      "source": [
        "model = LinearRegression(fit_intercept=True)                   # instantiate LinearRegression\n",
        "\n",
        "model.fit(x[:, np.newaxis], y)                                 # fit the model on data using 'x' as column vector\n",
        "\n",
        "xfit = np.linspace(0, 10, 1000)                                # create 1000 points between 0 and 10\n",
        "yfit = model.predict(xfit[:, np.newaxis])                      # predict the values for dependent variable \n",
        "\n",
        "plt.scatter(x, y, c='b')\n",
        "plt.plot(xfit, yfit, 'k');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJn4XLSIu9nG"
      },
      "source": [
        "### Learning Curves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4MyFLk0vIoh"
      },
      "source": [
        "Learning curve in machine learning is used to assess how models will perform with varying numbers of training samples.  This is achieved by monitoring the training and validation scores (model accuracy) with an increasing number of training samples.\n",
        "\n",
        "Below image showcases 'Learning curves representing high bias and high variance'.\n",
        "\n",
        "- orange dashed line - represent training \n",
        "- blue line - represent validation\n",
        "- black dashed line - desired model accuracy\n",
        "\n",
        "![wget](https://vitalflux.com/wp-content/uploads/2020/08/Screenshot-2020-08-19-at-6.22.21-AM-1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6KXB4hyceq-"
      },
      "source": [
        "### Example: Simple linear regression combined with the polynomial preprocessor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3H_xIbFsPiZ4"
      },
      "source": [
        "#### Polynomial Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwP_Fy7pPiZ4"
      },
      "source": [
        "Polynomial features are those features created by raising existing features to an exponent.\n",
        "\n",
        "For example, if a dataset had one input feature $X$, then a polynomial feature would be the addition of a new feature (column) where values were calculated by squaring the values in $X$, e.g. $X^2$. This process can be repeated for each input variable in the dataset, creating a transformed version of each. And, if an input sample is two dimensional and of the form $[a, b]$, the degree-2 polynomial features are $[1, a, b, a^2, ab, b^2]$.\n",
        "\n",
        "As such, polynomial features are a type of feature engineering, e.g. the creation of new input features based on the existing features.\n",
        "\n",
        "The “degree” of the polynomial is used to control the number of features added, e.g. a degree of 3 will add two new variables for each input variable. Typically a small degree is used such as 2 or 3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61V-KCWQzaLZ"
      },
      "source": [
        "def PolynomialRegression(degree=2, **kwargs):\n",
        "    return make_pipeline(PolynomialFeatures(degree), LinearRegression(**kwargs))   # using a pipeline to string these operations together "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiqvM00Nc8Ui"
      },
      "source": [
        "#### Creating data to fit into the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRWlwv-OzawV"
      },
      "source": [
        "def make_data(N, err=1.0, rseed=1):\n",
        "    # randomly sample the data\n",
        "    rng = np.random.RandomState(rseed)\n",
        "    X = rng.rand(N, 1) ** 2\n",
        "    y = 10 - 1. / (X.ravel() + 0.1)\n",
        "    if err > 0:\n",
        "        y += err * rng.randn(N)\n",
        "    return X, y\n",
        "\n",
        "X, y = make_data(40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2ldgnS9dRg7"
      },
      "source": [
        "#### Learning curve in Scikit-Learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo-sxZa0za4c"
      },
      "source": [
        "# learning curve for generated dataset with a second-order polynomial model and a ninth-order polynomial\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
        "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
        "\n",
        "for i, degree in enumerate([2, 9]):\n",
        "    N, train_lc, val_lc = learning_curve(PolynomialRegression(degree),\n",
        "                                         X, y, cv=7,\n",
        "                                         train_sizes=np.linspace(0.3, 1, 25))\n",
        "\n",
        "    ax[i].plot(N, np.mean(train_lc, 1), color='blue', label='training score')\n",
        "    ax[i].plot(N, np.mean(val_lc, 1), color='red', label='validation score')\n",
        "    ax[i].hlines(np.mean([train_lc[-1], val_lc[-1]]), N[0], N[-1],\n",
        "                 color='gray', linestyle='dashed')\n",
        "\n",
        "    ax[i].set_ylim(0, 1)\n",
        "    ax[i].set_xlim(N[0], N[-1])\n",
        "    ax[i].set_xlabel('training size')\n",
        "    ax[i].set_ylabel('score')\n",
        "    ax[i].set_title('degree = {0}'.format(degree), size=14)\n",
        "    ax[i].legend(loc='best')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CTMrSw9IOMr"
      },
      "source": [
        "### Example: Machine Learning Workflow using Linear-Regression with Iris-Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbdmV-KVJ1qT"
      },
      "source": [
        "In this example, we will be using the “Iris” dataset.\n",
        "\n",
        "The iris dataset contains the following data:\n",
        "\n",
        "50 samples of 3 different types of iris (150 samples total)\n",
        "\n",
        "<center>\n",
        "<img src=\"https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Machine+Learning+R/iris-machinelearning.png\" width= 600 px/>\n",
        "</center>\n",
        "\n",
        "Measurements: sepal length, sepal width, petal length, petal width\n",
        "\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/ritchieng/machine-learning-dataschool/master/images/03_iris.png\" width= 200 px/>\n",
        "</center>\n",
        "\n",
        "The format for the data: (sepal length, sepal width, petal length, petal width)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBdJEe00RzIn"
      },
      "source": [
        "#### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lseSO-6WFJq"
      },
      "source": [
        "The columns in this dataset are:\n",
        "\n",
        "* Id\n",
        "* SepalLengthCm\n",
        "* SepalWidthCm\n",
        "* PetalLengthCm\n",
        "* PetalWidthCm\n",
        "* Species\n",
        "\n",
        "Problem statement: Predict the sepal length (cm) of the iris flowers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CsrlII6I_dg"
      },
      "source": [
        "#### Loading Iris Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_riMt2ZI7x6"
      },
      "source": [
        "iris = datasets.load_iris()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pRoBT-Pwahu"
      },
      "source": [
        "Creating Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvjJjwSHetLk"
      },
      "source": [
        "iris_df = pd.DataFrame(data= iris.data, columns= iris.feature_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-keCy2FRhgZG"
      },
      "source": [
        "Displaying Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnIw_efHetO7"
      },
      "source": [
        "iris_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdCYEccaZ6CP"
      },
      "source": [
        "#### Exploring the iris dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3P75cdZaKWx"
      },
      "source": [
        "# print the names of the four features\n",
        "print (iris.feature_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CV1iKbJzYtnr"
      },
      "source": [
        "# print integers representing the Iris_Type of each observation\n",
        "# 0, 1, and 2 represent different Iris_Type\n",
        "print (iris.target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHegcd-YgEgf"
      },
      "source": [
        "#### Creating DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQ2IgHv0gEpW"
      },
      "source": [
        "iris_df = pd.DataFrame(data= iris.data, columns= iris.feature_names)\n",
        "target_df = pd.DataFrame(data= iris.target, columns= ['Iris_Type'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IrwcIiBgdxC"
      },
      "source": [
        "# coding categorical variables\n",
        "def converter(Iris_Type):\n",
        "    if Iris_Type == 0:\n",
        "        return 'setosa'\n",
        "    elif Iris_Type == 1:\n",
        "        return 'versicolor'\n",
        "    else:\n",
        "        return 'virginica'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdL60VnbggV9"
      },
      "source": [
        "target_df['Iris_Type'] = target_df['Iris_Type'].apply(converter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cUIDJpwgkbT"
      },
      "source": [
        "Concatenating the Dataframes (iris_df and target_df)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRFg6LuIgkjM"
      },
      "source": [
        "iris_df = pd.concat([iris_df, target_df], axis= 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9IkLj59gp6t"
      },
      "source": [
        "iris_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHo6vJqjgrsR"
      },
      "source": [
        "# generating descriptive statistics that summarize the central tendency, dispersion and shape of a dataset’s distribution, excluding NaN values.\n",
        "iris_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U999JB28g111"
      },
      "source": [
        "# summary of the DataFrame\n",
        "iris_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLOGj40ORa_v"
      },
      "source": [
        "### Visualization of Iris Data - Creating a pairplot of the iris data set to check which flower species seems to be most separable\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtN5qr4bRdFW"
      },
      "source": [
        "plt.style.use('ggplot')\n",
        "sns.pairplot(iris_df, hue= 'Iris_Type')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAfQTPD9hCVh"
      },
      "source": [
        "### Modeling and Prediction (Linear Regression)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw6BOS4vhCe2"
      },
      "source": [
        "# Converting Objects to Numerical dtype\n",
        "iris_df.drop('Iris_Type', axis= 1, inplace= True)\n",
        "target_df = pd.DataFrame(columns= ['Iris_Type'], data= iris.target)\n",
        "iris_df = pd.concat([iris_df, target_df], axis= 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL1XJhJyi_St"
      },
      "source": [
        "# Variables\n",
        "X= iris_df.drop(labels= 'sepal length (cm)', axis= 1)\n",
        "y= iris_df['sepal length (cm)']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W7S0AogjCWW"
      },
      "source": [
        "# Splitting the Dataset \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.33, random_state= 101)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oea-elTOjERk"
      },
      "source": [
        "# Instantiating LinearRegression() Model\n",
        "lr = LinearRegression()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0LvymXhjF3P"
      },
      "source": [
        "# Training/Fitting the Model\n",
        "lr.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfSXVgXejHZ1"
      },
      "source": [
        "# Making Predictions\n",
        "lr.predict(X_test)\n",
        "pred = lr.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVgqeN5VjI3D"
      },
      "source": [
        "# Evaluating Model's Performance\n",
        "print('Mean Absolute Error:', mean_absolute_error(y_test, pred))\n",
        "print('Mean Squared Error:', mean_squared_error(y_test, pred))\n",
        "print('Mean Root Squared Error:', np.sqrt(mean_squared_error(y_test, pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6w1N_t4jL73"
      },
      "source": [
        "Testing the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3bKV8HdjONM"
      },
      "source": [
        "iris_df.loc[6]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0ZQrEZ0jUfj"
      },
      "source": [
        "Predicting the value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4TkXYdQjWEh"
      },
      "source": [
        "pred = lr.predict(X_test)\n",
        "print('Predicted Sepal Length (cm):', pred[0])\n",
        "print('Actual Sepal Length (cm):', 4.6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5auVs4yrWK2"
      },
      "source": [
        "### Let us now apply the above learnings to perform a linear regression based price prediction, using a 'Real estate' dataset (Practice Ungraded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F12UbnoDlbG1"
      },
      "source": [
        "Linear regression model implementation\n",
        "\n",
        "  * Fit the model\n",
        "  * Do the prediction\n",
        "  * Plot the straight line for the predicted data using linear regression model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-Tfzn8hoYcL"
      },
      "source": [
        "#### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yokzc2fPoh9P"
      },
      "source": [
        "In this example, we will be using the “Real estate price prediction” dataset\n",
        "\n",
        "- Transaction date (purchase)\n",
        "- House age\n",
        "- Distance to the nearest MRT station (metric not defined)\n",
        "- Amount of convenience stores\n",
        "- Location (latitude and longitude)\n",
        "- House price of unit area\n",
        "\n",
        "Problem statement: Predict the house price of unit area based on various features provided such as house age, location, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA9pKMxUproB"
      },
      "source": [
        "#### Importing all the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAzxkjLKrEFC"
      },
      "source": [
        "# Your Code Here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtnD9bjTrEQv"
      },
      "source": [
        "#### Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FawXXiHk29Yl"
      },
      "source": [
        "df = pd.read_csv('Real_estate.csv')\n",
        "\n",
        "# Taking only the selected two attributes from the dataset\n",
        "df_binary = df[['X2 house age', 'Y house price of unit area']]\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIYaWysFsFR6"
      },
      "source": [
        "#### Dropping non-useful columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6m69YJtsFbt"
      },
      "source": [
        "#dropping columns\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtsNthCMx1HY"
      },
      "source": [
        "#### Finding if there are any null values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZRZ5EjZx1SH"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFpgaTDlsZEV"
      },
      "source": [
        "#### Exploring the data scatter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7o4RARCsZOa"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zHP3i7UwIW7"
      },
      "source": [
        "#### Training our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTtVP-7KwIgj"
      },
      "source": [
        "# Separating the data into independent and dependent variables\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "870vu2t0xObO"
      },
      "source": [
        "Splitting the data into training and testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dAkWC6ixO7s"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cba93Mk3xPRe"
      },
      "source": [
        "#### Training the Linear Regression model on the Training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUHU3Z53xP2W"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8Tc6Dffp1fV"
      },
      "source": [
        "Training/Fitting the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-1hJnFfp1xC"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7K4re2blxi17"
      },
      "source": [
        "#### Exploring the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoIWz8E8xjDg"
      },
      "source": [
        "# Data scatter of predicted values\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqyHyxSjVNzs"
      },
      "source": [
        "### Regularized Linear Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VViS8FUVVNtd"
      },
      "source": [
        "A good  way  to  reduce  overfitting  is  to  regularize  the model (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be for it to overfit the data. \n",
        "\n",
        "For a linear model, regularization is typically achieved by constraining the weights of the model. \n",
        "\n",
        "Three different ways to constrain the weights:\n",
        "\n",
        "1. Ridge Regression\n",
        "\n",
        "2. Lasso Regression\n",
        "\n",
        "3. Elastic Net\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FKiAkXmvWeG"
      },
      "source": [
        "#### **Ridge regression** \n",
        "\n",
        "**Ridge regression** or **Tikhonov regularization** is the regularization technique that performs L2 regularization. It modifies the loss function by adding the penalty (shrinkage quantity) equivalent to the square of the magnitude of coefficients.\n",
        "\n",
        "- **sklearn.linear_model.Ridge** is the module used to solve a regression model where the loss function is the linear least squares function and regularization is L2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC4iS7W0vz06"
      },
      "source": [
        "#### Ridge Regression with Scikit-Learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwJfyGNEoiME"
      },
      "source": [
        "n_samples, n_features = 15, 10\n",
        "rng = np.random.RandomState(0)\n",
        "y = rng.randn(n_samples)\n",
        "X = rng.randn(n_samples, n_features)\n",
        "\n",
        "rdg = linear_model.Ridge(alpha = 0.5)                  # instantiate Ridge regressor\n",
        "rdg.fit(X, y)\n",
        "rdg.score(X,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIHSJcX9rwQR"
      },
      "source": [
        "#### **LASSO (Least Absolute Shrinkage and Selection Operator)**\n",
        "\n",
        "LASSO is the regularisation technique that performs L1 regularisation. It modifies the loss function by adding the penalty (shrinkage quantity) equivalent to the summation of the absolute value of coefficients.\n",
        "\n",
        " - **sklearn.linear_model.Lasso** is a linear model, with an added regularisation term, used to estimate sparse coefficients.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEikDxywrwc0"
      },
      "source": [
        "#### Lasso Regression with Scikit-Learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_gWMgkv0bmh"
      },
      "source": [
        "# uses coordinate descent as the algorithm to fit the coefficients\n",
        " \n",
        "Lreg = linear_model.Lasso(alpha = 0.5)\n",
        "Lreg.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDnMWGBz2YoP"
      },
      "source": [
        "Once fitted, the model can predict new values as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmoW6yQ72bWg"
      },
      "source": [
        "Lreg.predict([[0,1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCoLlqp59xHQ"
      },
      "source": [
        "#weight vectors\n",
        "Lreg.coef_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdWkIuR-9xLl"
      },
      "source": [
        "#Calculating intercept\n",
        "Lreg.intercept_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FfgZS_79xPd"
      },
      "source": [
        "#Calculating number of iterations\n",
        "Lreg.n_iter_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1dQG-IsedbP"
      },
      "source": [
        "#### **Elastic-Net Regession**\n",
        "\n",
        "**Elastic-Net** is a regularised regression method that linearly combines both penalties i.e. L1 and L2 of the Lasso and Ridge regression methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLrFXQ015xAE"
      },
      "source": [
        "#### Elastic Net Regression with Scikit-Learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eji3z_d45xLk"
      },
      "source": [
        "# uses coordinate descent as the algorithm to fit the coefficients\n",
        "\n",
        "ENreg = linear_model.ElasticNet(alpha = 0.5,random_state = 0)\n",
        "ENreg.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2Oqk1Er5xS3"
      },
      "source": [
        "Once fitted, the model can predict new values as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvYzvIAP5xbr"
      },
      "source": [
        "ENreg.predict([[0,1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4irQt4aH9cxA"
      },
      "source": [
        "#weight vectors\n",
        "ENreg.coef_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTRFJdfK9kod"
      },
      "source": [
        "#Calculating intercept\n",
        "ENreg.intercept_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKP5O7P89okP"
      },
      "source": [
        "#Calculating number of iterations\n",
        "ENreg.n_iter_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOrlasNwlnPQ"
      },
      "source": [
        "### Understanding Significance of Alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ijjpzx_mLXd"
      },
      "source": [
        "Alpha is a parameter for regularization term, aka penalty term, that combats overfitting by constraining the size of the weights. Increasing alpha may fix high variance (a sign of overfitting) by encouraging smaller weights, resulting in a decision boundary plot that appears with lesser curvatures. Similarly, decreasing alpha may fix high bias (a sign of underfitting) by encouraging larger weights, potentially resulting in a more complicated decision boundary.\n",
        "\n",
        "Lasso regression is a common modeling technique to do regularization. So, we will be applying varying levels of alpha to show the effect on the coefficients.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPBNC5XQnC2l"
      },
      "source": [
        "Lreg = linear_model.Lasso(alpha = 0.25)\n",
        "Lreg.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n",
        "Lreg.coef_  #weight vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rilBFxGbnC2n"
      },
      "source": [
        "Lreg = linear_model.Lasso(alpha = 0.5)\n",
        "Lreg.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n",
        "Lreg.coef_  #weight vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkQ1FXEinSa9"
      },
      "source": [
        "Lreg = linear_model.Lasso(alpha = 0.75)\n",
        "Lreg.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n",
        "Lreg.coef_  #weight vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MeOAJYuoYeH"
      },
      "source": [
        "From the above 3 code cells, we could see that the coefficient value has been decreasing as the value of alpha is increased.\n",
        "\n",
        "\n",
        "\\begin{array} {|r|r|}\\hline AlphaValues & Coefficient Values \\\\\\hline 0.25 & 0.625 \\\\ \\hline .5 & .25 \\\\ \\hline .75 & 0. \\\\ \\hline  \\end{array}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUmztFX-Lh_v"
      },
      "source": [
        "### Theory Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVp0pNJoLkHY"
      },
      "source": [
        "\n",
        "1. What is the difference between the training set and the test set?\n",
        "\n",
        "    The training set is a subset of your data on which your model will learn how to predict the dependent variable with the independent variables.\n",
        "\n",
        "    The test set is the complimentary subset from the training set, on which you will evaluate your model to see if it manages to predict correctly the dependent variable with the independent variables.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3CmVNSiLibG"
      },
      "source": [
        "\n",
        "2. Why do we split on the dependent variable?\n",
        "\n",
        "    We want to have well-distributed values of the dependent variable in the training and test set. For example, if we only had the same value of the dependent variable in the training set, our model wouldn't be able to learn any correlation between the independent and dependent variables.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpQ2WyEBLiot"
      },
      "source": [
        "3. What is the purpose of a validation set?\n",
        "\n",
        "    The Validation Set is a separate section of your dataset that you will use during training to get a sense of how well your model is doing on data that are not being used in training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grpmkLdRLiyV"
      },
      "source": [
        "4. How do you choose the value of the regularization hyperparameter?\n",
        "\n",
        "      A common solution to this problem is called holdout validation: you simply hold out part of the training set to evaluate several candidate models and select the best one.\n",
        "      \n",
        "      The new held out set is called the validation set (or sometimes the development set, or dev set).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K481mDD4LjBl"
      },
      "source": [
        "5. If your model performs great on the training data but generalizes poorly to new instances, what is happening? Can you name three possible solutions?\n",
        "\n",
        "   If the model performs poorly to new instances, then it has overfitted on the training data. To solve this, we can do any of the following three: get more data, implement a simpler model, or eliminate outliers or noise from the existing data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgSwVENIPcM6"
      },
      "source": [
        "#@title What is the most common criterion used to determine the best-fitting line? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"\" #@param [\"\", \"The line that goes through the most points\", \"The line that has the same number of points above it as below it\", \"The line that minimizes the sum of squared errors of prediction\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzAZHt1zw-Y-",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}