{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M3_AST_03_Linear_SVM_C.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srilav/machinelearning/blob/main/M3_AST_03_Linear_SVM_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps9llghv8jX1"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "## A program by IISc and TalentSprint\n",
        "### Assignment 3: Linear Support Vector Machines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeP1PAXf8jYD"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkwaW3k58jYG"
      },
      "source": [
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* perform Linear SVM Classification\n",
        "* perform Support Vector Regression\n",
        "* visualize the boundary regions on the plane\n",
        "* visualize the regression line using Support Vector Regressor\n",
        "* compare LinearSVC, SVC and SGDClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoBCr7X7S0Jd"
      },
      "source": [
        "### Support Vector Machine - Classification (SVC)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVx8dO706ps4"
      },
      "source": [
        "**Support vector machines** are a set of supervised learning methods used for classification, regression, and outliers detection. \n",
        "\n",
        "A simple linear SVM classifier works by making a straight line between two classes. That means all of the data points on one side of the line will represent a category and the data points on the other side of the line will be put into a different category. This means there can be an infinite number of lines to choose from.\n",
        "\n",
        "**Hyperplanes** are decision boundaries that help classify the data points. Data points falling on either side of the hyperplane can be attributed to different classes.\n",
        " * The hyperplane with maximum margin is called the optimal hyperplane.\n",
        "\n",
        "**Support vectors** are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. \n",
        "\n",
        "**Margin** is the width that the boundary could be increased by before hitting a data point.\n",
        "\n",
        "![wget](https://cdn.talentsprint.com/aiml/aiml_2020_b14_hyd/experiment_details_backup/linear_data.png)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1RkdZga9gOG"
      },
      "source": [
        "### Implementing SVM classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H424-SLpx2Cn"
      },
      "source": [
        "#### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZEjjm2j0Zes"
      },
      "source": [
        "The dataset used in this assignment is `Car_Advertisement` consists of data collected by a car retail company on their previous customers.\n",
        "\n",
        "The datafields are:\n",
        "\n",
        "* User ID \n",
        "* Gender\n",
        "* Age\n",
        "* Salary\n",
        "* Purchased (whether or not they purchased a car - this is the target for our predictions) \n",
        "\n",
        "Problem statement: Using the data, we will try to predict if a new customer will purchase a car based on their gender, age and salary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YzfoPvJDiTX"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjoZJWGErxGf"
      },
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBPPuGmBlDIN",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook= \"M3_AST_03_Linear_SVM_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")  \n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/CDS/Datasets/Car_Advertisement.csv\")\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/CDS/Datasets/position_salaries.csv\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "    \n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None        \n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "    \n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None   \n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://cds.iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "    \n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional: \n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional  \n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "  \n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "  \n",
        "  \n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "  \n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getId():\n",
        "  try: \n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup \n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup() \n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wm7UZLmTm5WX"
      },
      "source": [
        "### Importing required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w32Q0JsNn1Ei",
        "scrolled": true
      },
      "source": [
        "import numpy as np                                                        # basic library to work with arrays\n",
        "import pandas as pd                                                       # to read files\n",
        "import seaborn as sns                                                     # library for statistical data visualization\n",
        "import matplotlib.pyplot as plt                                           # basic library for plotting graphs and visualization\n",
        "from matplotlib.colors import ListedColormap                              # for filling colors in mapping\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score              # importing confusion matrix, accuracy score\n",
        "from sklearn.svm import SVC, SVR, LinearSVC                               # importing Support vector classifier, Support Vector Regressor, LinearSVC  \n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder            # preprocessing\n",
        "from sklearn.model_selection import train_test_split                      # for splitting the dataset\n",
        "from mlxtend.plotting import plot_decision_regions                        # to plot the decision boundaries and hyperplane\n",
        "from sklearn.pipeline import make_pipeline                                # to import pipeline\n",
        "from sklearn.linear_model import SGDClassifier                            # to import SGD Classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjjVoy_m3ZN9"
      },
      "source": [
        "#### Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYhKwe3mx0UP"
      },
      "source": [
        "dataset= pd.read_csv('Car_Advertisement.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0AW_ib3CErI"
      },
      "source": [
        "# converting categorical variables and assigning 0 and 1\n",
        "def converter(Purchased):\n",
        "    if Purchased == 'Not Purchased':\n",
        "        return 0\n",
        "    else:\n",
        "       Purchased == 'Purchased'\n",
        "       return 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B-lXkUeCuz-"
      },
      "source": [
        "# apply converter function to update the 'Purchased' after assigning 0 and 1\n",
        "dataset['Purchased'] = dataset['Purchased'].apply(converter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjW2cmayP0Md"
      },
      "source": [
        "# first five rows of dataset\n",
        "dataset.head()        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufiS_Q2_P0Ml"
      },
      "source": [
        "dataset.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDDm9sRUS0KK"
      },
      "source": [
        "The dataset contains 400 rows and 5 attributes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVkdAWMxMSY-"
      },
      "source": [
        "#### Extracting Independent and dependent Variable  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ay2S_u4x0di"
      },
      "source": [
        "# defining the independent and dependent variable\n",
        "x = dataset.iloc[:, [2,3]].values                  \n",
        "y = dataset.iloc[:, 4].values                      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLGvu_AD6oxh"
      },
      "source": [
        "#### Splitting the dataset into training and test set.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFEsWWlUx0i2"
      },
      "source": [
        "# using train test split, splitting the dataset into training and testing data \n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.25, random_state=0)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_dlSssD6rl2"
      },
      "source": [
        "#### Feature Scaling  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IDQ36tex0oA"
      },
      "source": [
        "# Scaling the data\n",
        "st_x = StandardScaler()    \n",
        "x_train = st_x.fit_transform(x_train)    \n",
        "x_test = st_x.transform(x_test)       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVHMLRKn6-cf"
      },
      "source": [
        "#### Fitting the SVM classifier to the training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmPOtdPdx0tO"
      },
      "source": [
        "# kernel = 'linear' is used on data which is separable\n",
        "classifier = SVC(kernel='linear', random_state=0)  \n",
        "classifier.fit(x_train, y_train)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZtLCSuDS0Kb"
      },
      "source": [
        "Let us also fit the data using **LinearSVC** and compare it with **SVC(kernel='linear')** performed above. Learn about their differences [here](https://intellipaat.com/community/19783/which-one-is-better-linearsvc-or-svc)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4N4s3eGzS0Kd"
      },
      "source": [
        "# Make a pipeline of Scaling and SVM\n",
        "clf = make_pipeline(StandardScaler(),\n",
        "                     LinearSVC(random_state=0, tol=1e-5))\n",
        "clf.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4edLirq07ZIM"
      },
      "source": [
        "#### Predicting the test set result  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb2HbkDyx00d"
      },
      "source": [
        "# predicting using SVC with kernel='Linear'\n",
        "y_pred = classifier.predict(x_test)   \n",
        "print(\"predicted values \",y_pred)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5DlJOvDS0Ki"
      },
      "source": [
        "# predicting using LinearSVC\n",
        "y_pred1 = clf.predict(x_test)      \n",
        "print(\"predicted values \",y_pred1)     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcEoA_997jN-"
      },
      "source": [
        "#### Creating the Confusion matrix  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0EePfU2S0Kn"
      },
      "source": [
        "A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqwN7nvl7jZd"
      },
      "source": [
        "# confusion matrix between true values and predicted outcomes for first model\n",
        "cm = confusion_matrix(y_test, y_pred)   \n",
        "cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96_6y1ENFKTq"
      },
      "source": [
        "# Heatmap of SVC with kernel as linear\n",
        "labels = ['Truely Not Purchased 66','Falsely Purchased 2','Falsely Not Purchased 8','Truely Purchased 24']\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sns.heatmap(cm, annot=labels, fmt='', cmap='Blues')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2kQCgypS0Ks"
      },
      "source": [
        "# creating confusion matrix between true values and predicted outcomes for second model\n",
        "cm1 = confusion_matrix(y_test, y_pred1)   \n",
        "cm1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVW0hHG8S0Kw"
      },
      "source": [
        "# Heatmap of LinearSVC\n",
        "labels = ['Truely Not Purchased 66','Falsely Purchased 2','Falsely Not Purchased 8','Truely Purchased 24']\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sns.heatmap(cm1, annot=labels, fmt='', cmap='Blues')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87SrYt1CS0Kz"
      },
      "source": [
        "Here, 66 are showing a true positive value and 24 are showing a true negative value for both the models.\n",
        "\n",
        "So, we can say that both of them are giving the same predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bj0EgeK7wWF"
      },
      "source": [
        "#### Visualizing the training set result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZLV951D7wjj"
      },
      "source": [
        "x_set, y_set = x_train, y_train  \n",
        "# Creating a meshgrid\n",
        "x1, x2 = np.meshgrid(np.arange(start = x_set[:, 0].min() - 1, stop = x_set[:, 0].max() + 1, step  =0.01), \n",
        "                     np.arange(start = x_set[:, 1].min() - 1, stop = x_set[:, 1].max() + 1, step = 0.01))\n",
        "                           \n",
        "plt.contourf(x1, x2, classifier.predict(np.array([x1.ravel(), x2.ravel()]).T).reshape(x1.shape),  \n",
        "alpha = 0.75, cmap = ListedColormap(('green', 'red')))                                                                  # plotting the contour lines here\n",
        "plt.xlim(x1.min(), x1.max())  \n",
        "plt.ylim(x2.min(), x2.max())  \n",
        "# defining a for loop for segregating the purchased as 0 and 1\n",
        "for i, j in enumerate(np.unique(y_set)):  \n",
        "    plt.scatter(x_set[y_set == j, 0], x_set[y_set == j, 1],  \n",
        "        c = ListedColormap(('green', 'red'))(i), label = j)                                                             # giving the green label to all 0 and red label to all 1\n",
        "plt.title('SVM classifier (Training set)')  \n",
        "plt.xlabel('Scaled Age')  \n",
        "plt.ylabel('Scaled Estimated Salary')  \n",
        "plt.legend()  \n",
        "plt.show()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boypcmqT8AmS"
      },
      "source": [
        "In the output, we got the straight line as a hyperplane because we have used a linear kernel in the classifier.\n",
        "\n",
        "To know more about meshgrid, click [here](https://numpy.org/doc/stable/reference/generated/numpy.meshgrid.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nktvqGKy7wu1"
      },
      "source": [
        "#### Visualizing the test set result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY3-bdJ-S0K6"
      },
      "source": [
        "Just similar to above, here we are visualizing the test results or predicted results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKoO7m2T7w5K"
      },
      "source": [
        "x_set, y_set = x_test, y_test  \n",
        "# Creating a meshgrid\n",
        "x1, x2 = np.meshgrid(np.arange(start = x_set[:, 0].min() - 1, stop = x_set[:, 0].max() + 1, step  =0.01), \n",
        "                     np.arange(start = x_set[:, 1].min() - 1, stop = x_set[:, 1].max() + 1, step = 0.01))\n",
        "  \n",
        "plt.contourf(x1, x2, classifier.predict(np.array([x1.ravel(), x2.ravel()]).T).reshape(x1.shape),  \n",
        "alpha = 0.75, cmap = ListedColormap(('green','red' )))  \n",
        "plt.xlim(x1.min(), x1.max())  \n",
        "plt.ylim(x2.min(), x2.max())  \n",
        "for i, j in enumerate(np.unique(y_set)):  \n",
        "    plt.scatter(x_set[y_set == j, 0], x_set[y_set == j, 1],  \n",
        "        c = ListedColormap(('green', 'red'))(i), label = j)  \n",
        "plt.title('SVM classifier (Test set)')  \n",
        "plt.xlabel('Age')  \n",
        "plt.ylabel('Estimated Salary')  \n",
        "plt.legend()  \n",
        "plt.show()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66VvedRx7xCz"
      },
      "source": [
        "In the above output image, the SVM classifier has divided the users into two regions (Purchased or Not purchased). Users who purchased the SUV are in the red region with the red scatter points. And users who did not purchase the SUV are in the green region with green scatter points. The hyperplane has divided the two classes into Purchased and Not purchased variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZ4RkvPpS0K_"
      },
      "source": [
        "# calculating the accuracy \n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tctt7JsLS0LC"
      },
      "source": [
        "Thus, we get an accuracy of 90% on testing data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoX6cqLoS0LE"
      },
      "source": [
        "### Implementing SGD Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2tHAhU8S0LG"
      },
      "source": [
        "Linear classifiers (SVM, logistic regression, etc.) with SGD training.\n",
        "\n",
        "This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing learning rate. SGD allows minibatch (online/out-of-core) learning via the `partial_fit` method. For best results using the default learning rate schedule, the data should have zero mean and unit variance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L36Hgoo3S0LJ"
      },
      "source": [
        "# Always scale the input. The most convenient way is to use a pipeline.\n",
        "clf2 = make_pipeline(StandardScaler(),\n",
        "                     SGDClassifier(max_iter=1000, tol=1e-3, loss=\"hinge\"))\n",
        "clf2.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1lQiJ27S0LL"
      },
      "source": [
        "**Note:** By default, loss is \"hinge\". It gives a Linear SVM. So, here we can see that if there are any changes in the predictions using SGD Classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TadCptPS0LN"
      },
      "source": [
        "#### Predicting Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H253XqKwS0LO"
      },
      "source": [
        "# predicting results using SGD Classifier\n",
        "y_pred2 = clf2.predict(x_test)              "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oohJSLE_S0LP"
      },
      "source": [
        "y_pred2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TfSUVOXS0LQ"
      },
      "source": [
        "#### Creating Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VGcmGURS0LS"
      },
      "source": [
        "# confusion matrix between true values and predicted outcomes for SGD Classifier\n",
        "cm2 = confusion_matrix(y_test, y_pred2)    \n",
        "cm2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4Y2JGP_S0LU"
      },
      "source": [
        "# Heatmap of sgd classifier\n",
        "labels = ['Truely Not Purchased 64','Falsely Purchased 2','Falsely Not Purchased 9','Truely Purchased 23']\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sns.heatmap(cm2, annot=labels, fmt='', cmap='Reds')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa0GP-Z7S0LW"
      },
      "source": [
        "#### Calculating Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64ft9OjxS0LX"
      },
      "source": [
        "accuracy_score(y_test, y_pred2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVTergBSS0LZ"
      },
      "source": [
        "Using SGD Classifier with loss=\"hinge\", we get an accuracy of 87%. \n",
        "\n",
        "To know more about SGDClassifier, click [here](https://michael-fuchs-python.netlify.app/2019/11/11/introduction-to-sgd-classifier/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EudjjWh3S0Lb"
      },
      "source": [
        "### Support Vector Machine - Regression (SVR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TIrW3K2S0Lc"
      },
      "source": [
        "To use SVMs for regression instead of classification, the trick is to reverse the objective: instead of trying to fit the largest possible street between two classes while limiting\n",
        "margin violations, SVM Regression tries to fit as many instances as possible\n",
        "on the street while limiting margin violations (i.e., instances off the street). The width of the street is controlled by a hyperparameter, ε\n",
        "\n",
        "![Image](https://www.saedsayad.com/images/SVR_2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_mJJ1GaljmT"
      },
      "source": [
        "### Implementing Support Vector Regression (SVR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I83Xkd6Hl5ez"
      },
      "source": [
        "#### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDSteTRZmAs6"
      },
      "source": [
        "The dataset (Position_Salaries) consists of a list of positions in a company along with the band levels and their associated salary.\n",
        "\n",
        "Information Used To Predict Salaries:\n",
        "\n",
        "* Job Type: The position held (Business Analyst, Junior Consultant, Senior Consultant….)\n",
        "* Position Level: Experience in years (1,2,3…..)\n",
        "\n",
        "**Problem Statement**\n",
        "\n",
        "To predict the accurate salary of an employee having a level 6.5, using SVM regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db5HtVgq4KJv"
      },
      "source": [
        "#### Importing the datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKlj4XjW6p7E"
      },
      "source": [
        "dataset = pd.read_csv('position_salaries.csv')\n",
        "#  defining independent and dependent variables\n",
        "X = dataset.iloc[:, 1:-1].values                        \n",
        "y = dataset.iloc[:, -1].values                         "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XY8EmJmR__4"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_lhw9NZSAeQ"
      },
      "source": [
        "dataset.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Zro-tpX6qCw"
      },
      "source": [
        "#### Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78UJvI5D26RL"
      },
      "source": [
        "# using train test split, splitting the dataset into training and testing data\n",
        "X_Train, X_Test, y_Train, y_Test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKNC3ymp6qK4"
      },
      "source": [
        "print(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpppEKjT0Bmn"
      },
      "source": [
        "y = y.reshape(len(y),1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yROB4Od30DaB"
      },
      "source": [
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LDqvFgs6qZI"
      },
      "source": [
        "#### Feature Scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nourDX4tS0Lv"
      },
      "source": [
        "Feature scaling is a method used to normalize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd60LL_h0K7w"
      },
      "source": [
        "# Scaling the data\n",
        "sc_X = StandardScaler()\n",
        "sc_y = StandardScaler()\n",
        "X = sc_X.fit_transform(X)\n",
        "y = sc_y.fit_transform(y)\n",
        "print(X)   \n",
        "print(y)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjIKcoBc0UV8"
      },
      "source": [
        "#### Training the SVR model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXeJttsD0Vmh"
      },
      "source": [
        "# defining the SVR model\n",
        "regressor = SVR(kernel = 'rbf')        \n",
        "regressor.fit(X, y)                    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdOHNct0S0L5"
      },
      "source": [
        "To know about .fit(), click [here](https://towardsdatascience.com/fit-vs-predict-vs-fit-predict-in-python-scikit-learn-f15a34a8d39f)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFJu2DUw0eoI"
      },
      "source": [
        "#### Predicting new Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hfVVHDQS0L8"
      },
      "source": [
        "Here we will predict the new salary according to 6.5 level,i.e., 6.5 years of experience."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veHzx9MC0VqJ"
      },
      "source": [
        "# Reversing the scaling of y.\n",
        "sc_y.inverse_transform(regressor.predict(sc_X.transform([[6.5]])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkaNEF-e0hzJ"
      },
      "source": [
        "#### Visualizing the SVR results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1xnTU0c0mPL"
      },
      "source": [
        " # plotting Real Salaries as red dot scatter plot\n",
        "plt.scatter(sc_X.inverse_transform(X), sc_y.inverse_transform(y), color = \"red\")                   \n",
        "# plotting the Predicted salaries by SVR Model\n",
        "plt.plot(sc_X.inverse_transform(X), sc_y.inverse_transform(regressor.predict(X)), color = \"blue\")   \n",
        "plt.title(\"Support Vector Regression\")                                                              \n",
        "plt.xlabel(\"Position Level\")                                                                        \n",
        "plt.ylabel(\"Salary\")   \n",
        "plt.show()                                                                             "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ehrx1hgFCGbr"
      },
      "source": [
        "In the output- we have the best fit line that has the maximum number of points. The red dots show the actual dataset points and the blue line is the best fit obtained using SVM regressor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIPsZBHH0h1k"
      },
      "source": [
        "#### Visualizing the SVR result (for higher resolution and smoother curve)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nS6rzBYx0h-m"
      },
      "source": [
        "# make a grid of features from minimum to maximum value with a step of 0.1\n",
        "X_grid = np.arange(min(sc_X.inverse_transform(X)), max(sc_X.inverse_transform(X)), 0.1)    \n",
        "# reshaping the X_grid to a column              \n",
        "X_grid = X_grid.reshape((len(X_grid), 1))                                            \n",
        "# plotting the actual values of salary as red dot                   \n",
        "plt.scatter(sc_X.inverse_transform(X), sc_y.inverse_transform(y), color = 'red')   \n",
        "# plotting the predicted values of salary on a blue line (best fit line)                      \n",
        "plt.plot(X_grid, sc_y.inverse_transform(regressor.predict(sc_X.transform(X_grid))), color = 'blue')      \n",
        "plt.title('SVR')                                                                                         \n",
        "plt.xlabel('Position level')                                                                             \n",
        "plt.ylabel('Salary')                                                                                     \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COf2aF5AS0MF"
      },
      "source": [
        "Here, we obtain a more smooth fit line."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtGvBPSUXKmx"
      },
      "source": [
        "### Theory Questions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3uie7DnX1dG"
      },
      "source": [
        "1. What is the fundamental idea behind Support Vector Machines?\n",
        "\n",
        "  The fundamental idea behind Support Vector Machines is to fit the widest possible \"street\" between the classes. In other words, the goal is to have the largest possible margin between the decision boundary that separates the two classes and the training instances. \n",
        "  \n",
        "  When performing soft margin classification, the SVM searches for a compromise between perfectly separating the two classes and having the widest possible street (i.e., a few instances may end up on the street)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew3rC6wkXK2W"
      },
      "source": [
        "2. What is a support vector?\n",
        "\n",
        "  After training an SVM, a support vector is any instance located on the \"street\", including its border. The decision boundary is entirely determined by the support vectors. Any instance that is not a support vector (i.e., off the street) has no influence whatsoever. Computing the predictions only involve the support vectors, not the whole training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuV-_WLsXLHf"
      },
      "source": [
        "3. Why is it important to scale the inputs when using SVMs?\n",
        "\n",
        "  SVMs try to fit the largest possible \"street\" between the classes, so if the training set is not scaled, the SVM will tend to neglect small features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UHpaH_QXLav"
      },
      "source": [
        "4. Can an SVM classifier output a confidence score when it classifies an instance? What about probability?\n",
        "\n",
        "  An SVM classifier can output the distance between the test instance and the decision boundary, and you can use this as a confidence score. However, this score cannot be directly converted into an estimation of the class probability. If you set probability=True when creating an SVM in Scikit-Learn, then after training it will calibrate the probabilities using Logistic Regression on the SVM's scores. This will add the predict_proba() and predict_log_proba() methods to the SVM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWzQKOauXL3n"
      },
      "source": [
        "5. How does SVM regression work?\n",
        "\n",
        "  You reverse the objective: instead of trying to fit the largest possible street between two classes while limiting margin violations, SVM Regression tries to fit as many instances as possible on the street while limiting margin violations (i.e., instances off the street).\n",
        "\n",
        "  The idea is that the decision boundary becomes the prediction function and you want it to be as close as possible to the values of the instances.\n",
        "The width of the street is controlled by a hyperparameter epsilon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgSwVENIPcM6"
      },
      "source": [
        "#@title Select the FALSE statement w.r.t an SVM classifier: { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"\" #@param [\"\", \"It makes a decision boundary in such a way that the separation between the two classes is as wide as possible\", \"It finds the points closest to the line from both the classes. These points are called support vectors\", \"The objective of the algorithm is to minimize the margin i.e. the distance between the line and the support vectors\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzAZHt1zw-Y-",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}